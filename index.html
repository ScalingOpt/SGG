<!DOCTYPE html>
<html>

<head>
  <link rel="icon" type="image/png" href="website/img/page_logo.png">

  <title>SGG: Taming LLMs by Scaling Learning Rates with Gradient Grouping</title>

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.0/dist/chart.umd.min.js   "></script>
    <script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-datalabels@2.0.0   "></script>
    <script
        src="https://cdn.jsdelivr.net/npm/chartjs-plugin-annotation@3.0.1/dist/chartjs-plugin-annotation.min.js   "></script>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans   |Noto+Sans|Castoro" rel="stylesheet">
    <link rel="stylesheet" href="website/css/bulma.min.css">
    <link rel="stylesheet" href="website/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="website/css/bulma-slider.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css   ">
    <link rel="stylesheet" href="website/css/fontawesome.all.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js   "></script>
    <script src="website/javascript/bulma-carousel.min.js"></script>
    <script src="website/javascript/bulma-slider.min.js"></script>
    <script src="website/javascript/explorer-index.js"></script>

    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css   " rel="stylesheet"
        integrity="sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js   "
        integrity="sha384-ka7Sk0Gln4gmtz2MlQnikT1wXgYsOg+OMhuP+IlRH9sENBO0LRn5q+8nbTov4+1p"
        crossorigin="anonymous"></script>

    <link href="https://unpkg.com/tabulator-tables@5.5.2/dist/css/tabulator_bootstrap4.min.css   " rel="stylesheet">
    <script type="text/javascript" src="https://unpkg.com/tabulator-tables@5.5.2/dist/js/tabulator.min.js   "></script>
    <script defer src="website/javascript/fontawesome.all.min.js"></script>
    <!-- <script src="website/javascript/peity-vanilla.js"></script> -->


    <!-- below we load some js scripts -->
    <script src="website/javascript/benchmark_table.js" type="module"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6   "></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js   "></script>

    <link rel="stylesheet" href="website/css/index.css">

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C7GJ4FYMY9   "></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());

        gtag('config', 'G-C7GJ4FYMY9');
    </script>

    <!-- MathJax script -->
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML   ">
    </script>
    <script type="text/javascript">
        MathJax.Hub.Config({
            tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
        });
    </script>

    <noscript>
        <p><img alt="Clicky" width="1" height="1" src="https://in.getclicky.com/101339888ns.gif   " /></p>
    </noscript>

    <script>
        document.addEventListener('DOMContentLoaded', function () {
        var toggles = document.querySelectorAll('.toggle-section');
        toggles.forEach(function(toggle) {
            toggle.addEventListener('click', function() {
            var content = document.getElementById(toggle.getAttribute('aria-controls'));
            var toggleIcon = toggle.children[1].children[0];
            content.classList.toggle('is-active');
            if (content.classList.contains('is-active')) {
                toggleIcon.style.transition = 'transform 0.3s ease';
                toggleIcon.style.transform = 'rotate(180deg)';
            } else {
                toggleIcon.style.transition = 'transform 0.3s ease';
                toggleIcon.style.transform = 'rotate(0deg)';
            }
            });
        });
        });
      </script>

    <style>
        .collapse-content {
          display: none;
          margin-top: 10px;
        }
        .collapse-content.is-active {
          display: block;
        }
        /* .toggle-section .icon.is-small {
          transition: transform 0.3s ease;
        } */
        /* .toggle-section .fa-angle-up {
          transform: rotate(180deg);
        } */
      </style>
</head>

<body>

  <!-- Single-line Title with Bulma's .title.is-1 is-bold (centered heading) -->
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">

            <!-- MAIN TITLE (logo + "AlphaOne:") -->
            <!-- TITLE -->
            <h1 class="title is-1 publication-title is-bold" style="display: flex; align-items: center; justify-content: center; gap: 10px;">
              <!-- Left: ACL Image -->
              <img src="website/img/acl.png" alt="ACL Logo" style="height: 1em; vertical-align: middle;" />

              <!-- Center: SGG Text -->
              <span class="alphaone" style="vertical-align: middle;">SGG</span>

              <!-- Right: 2025 Main * Label -->
              <span style="font-size: 0.6em; color: #555; vertical-align: middle;">2025 Main ⭐️</span>
            </h1>

            <!-- SUBTITLE -->
            <h2 class="title is-1 publication-title is-bold" style="margin-top: -0.5em; margin-right: 30px; margin-left: 30px; text-align: center;">
              Taming LLMs by Scaling Learning Rates with Gradient Grouping
            </h2>
            <!-- Author list -->
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://lupin1998.github.io   "   target="_blank">Siyuan Li*</a>
                <sup style="display: inline-flex; align-items: center; margin-left: -0.3em; gap: 0.05em;">
                  <img src="website/img/zhejiang.png" alt="zhejiang" style="height: 1.2em; vertical-align: baseline; margin: 0 0.1em;">
                </sup>&nbsp;&nbsp;&nbsp;
              </span>
              <span class="author-block">
                <a href="https://github.com/tianshijing   "   target="_blank">Juanxi Tian*</a>
                <sup style="display: inline-flex; align-items: center; margin-left: -0.3em; gap: 0.05em;">
                  <img src="website/img/pku.png" alt="pku" style="height: 1.2em; vertical-align: baseline; margin: 0 0.1em;">
                </sup>&nbsp;&nbsp;&nbsp;
              </span>
              <span class="author-block">
                <a href="https://jacky1128.github.io   "   target="_blank">Zedong Wang*</a>
                <sup style="display: inline-flex; align-items: center; margin-left: -0.3em; gap: 0.05em;">
                  <img src="website/img/hkust.png" alt="hkust" style="height: 1.2em; vertical-align: baseline; margin: 0 0.1em;">
                </sup>&nbsp;&nbsp;&nbsp;
              </span>
              <span class="author-block">
                <a href="https://jinxins.github.io   "   target="_blank">Xin Jin</a>
                <sup style="display: inline-flex; align-items: center; margin-left: -0.3em; gap: 0.05em;">
                  <img src="website/img/westlake.png" alt="westlake" style="height: 1.2em; vertical-align: baseline; margin: 0 0.1em;">
                </sup>&nbsp;&nbsp;&nbsp;
              </span><br>
              <span class="author-block">
                <a href="">Zicheng Liu</a>
                <sup style="display: inline-flex; align-items: center; margin-left: -0.3em; gap: 0.05em;">
                  <img src="website/img/zhejiang.png" alt="zhejiang" style="height: 1.2em; vertical-align: baseline; margin: 0 0.1em;">
                </sup>&nbsp;&nbsp;&nbsp;
              </span>
              <span class="author-block">
                <a href="https://zwt233.github.io   ">Wentao Zhang</a>
                <sup style="display: inline-flex; align-items: center; margin-left: -0.3em; gap: 0.05em;">
                  <img src="website/img/pku.png" alt="pku" style="height: 1.2em; vertical-align: baseline; margin: 0 0.1em;">
                </sup>&nbsp;&nbsp;&nbsp;                
              </span>
              <span class="author-block">
                <a href="https://www.danxurgb.net   ">Dan Xu</a>
                <sup style="display: inline-flex; align-items: center; margin-left: -0.3em; gap: 0.05em;">
                  <img src="website/img/hkust.png" alt="hkust" style="height: 1.2em; vertical-align: baseline; margin: 0 0.1em;">
                </sup>&nbsp;&nbsp;&nbsp;
              </span>
            </div>
            <div class="is-size-5 publication-authors" style="margin-top:1em;">
              <span class="author-block">
                <sup style="display: inline-flex; align-items: center; margin-left: -0.3em; gap: 0.05em;">
                  <img src="website/img/zhejiang.png" alt="zhejiang" style="height: 1.2em; vertical-align: baseline; margin: 0 0.1em;">
                </sup>Zhejiang University</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
              <span class="author-block">
                <sup style="display: inline-flex; align-items: center; margin-left: -0.3em; gap: 0.05em;">
                  <img src="website/img/westlake.png" alt="westlake" style="height: 1.2em; vertical-align: baseline; margin: 0 0.1em;">
                </sup>Westlake University</span>
              <span class="author-block">
                <sup style="display: inline-flex; align-items: center; margin-left: -0.3em; gap: 0.05em;">
                  <img src="website/img/hkust.png" alt="hkust" style="height: 1.2em; vertical-align: baseline; margin: 0 0.1em;">
                </sup>The Hong Kong University of Science and Technology</span>
              <span class="author-block">
                <sup style="display: inline-flex; align-items: center; margin-left: -0.3em; gap: 0.05em;">
                  <img src="website/img/pku.png" alt="pku" style="height: 1.2em; vertical-align: baseline; margin: 0 0.1em;">
                </sup>Peking University</span>
            </div>
            <!-- Links -->
            <div class="column has-text-centered" style="margin-top:1.5em;">
              <span class="link-block">
                <a href="https://arxiv.org/abs/2506.01049   " class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>Paper</span>
                </a>&nbsp;&nbsp;
              </span>
              <span class="link-block">
                <a href="https://github.com/ScalingOpt/SGG   " class="external-link button is-normal is-rounded is-dark" role="button" target="_blank">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code (Coming soon)</span>
                </a> &nbsp;&nbsp;
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container" style="margin-top: -6rem; margin-bottom: -4rem; max-width: 80%;">
      <div class="columns is-centered m-6">
        <div class="column is-full">
          <!-- 使用 flex 布局保持内容居中 -->
          <div class="box p-4" style="width: 80%; height: 90%; margin: 0 auto; display: flex; align-items: center;">
            <div class="columns is-mobile is-vcentered" style="display: table-row; margin: 0 auto; width: 100%;">
              <!-- 图片列 -->
              <div class="column is-half" style="display: table-cell; vertical-align: middle;">
                <div style="text-align: right; padding-right: -1rem;">
                  <img src="website/img/SGG_1.png" alt="alphaone-teaser" 
                       style="max-width: 90%; margin-right: -0.3rem;" />
                  <!-- 添加 caption -->
                  <p style="font-size: 0.9rem; color: #555; margin-top: 0.5rem; text-align: center;">
                    Scaling with Gradient Grouping. Illustration of SGG with online grouping and group-specific learning rate (LR) scaling upon adaptive LR optimizers.
                  </p>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
<!-- <p style="text-align: left; margin: 0; color:rgb(60, 56, 56);"> 
                  <b>Figure 1.</b> <b>Conceptual illustration</b> of reasoning modulation strategies. 
                  Our <span style="font-size: 0.9em;">\(\alpha\)1</span> employs a <i>slow-to-fast</i> reasoning schedule controlled by <span style="font-size: 0.9em;">\(\alpha\)</span>. 
                  <span style="font-size: 0.9em;">\(\alpha\)1</span> scales more efficiently than <i>monotonously increasing</i> method s1 (<span style="color: #dbbf65; font-weight: bold; font-size: 1.0em;">yellow</span>) and generally outperforms <i>monotonously decreasing</i> (<span style="color: #a36ca0; font-weight: bold; font-size: 1.0em;">purple</span>) approaches.
                </p> -->
  <!-- Overview Section -->
  <section class="section" id="overview" style="padding-top:1.5rem;">
    <div class="container" style="max-width: 80%; margin: 0 auto;">
      <h1 class="title is-3" style="text-align:center; font-size: 2.3em; margin-bottom:1rem;">Abstract</h1>
      <br>
      <div class="content has-text-justified" style="max-width: 85%; font-size: 1.2em; margin: 0 auto; line-height: 1.55;">
        <p style="text-align:justify; color:rgb(31, 30, 30);">
          Training large language models (LLMs) poses challenges due to their massive scale and heterogeneous architectures. While adaptive optimizers like AdamW help address gradient variations, they still struggle with efficient and effective parameter-wise learning rate estimation, resulting in training instability, slow convergence, and poor compatibility with parameter-efficient fine-tuning (PEFT) techniques. This work introduces <span class="sgg">Scaling with Gradient Grouping</span> (SGG), an optimizer wrapper that improves adaptive learning rate estimation by dynamic grouping and group-specific scaling. SGG first groups gradient statistics in each layer into clusters and then applies cluster-specific scaling to calibrate learning rates for each parameter, thus imposing collective group-wise constraints while maintaining precise per-parameter adaptation.
        </p>
  
      </div>
    </div>
  </section>

<section class="section" id="Method" style="padding-top:1.5rem;">
    <div class="container" style="max-width: 80%; margin: 0 auto;">
        <h1 class="title is-3" style="text-align:center; font-size: 2.3em; margin-bottom:1rem;">Method</h1>
        <br>
      <div style="text-align: center; margin: 2rem 0;">
        <img src="website/img/SGG_2.png" alt="SGG Framework Overview" style="width: 40%; height: auto; display: inline-block;">
      </div>
        <div class="content has-text-justified" style="max-width: 85%; font-size: 1.2em; margin: 0 auto; line-height: 1.55;">
            <p style="text-align:justify; color:rgb(31, 30, 30);">
                The key steps of SGG are as follows:
            </p>
            <ol style="text-align:justify; color:rgb(31, 30, 30);">
                <li>Dynamic Gradient Grouping: SGG dynamically clusters gradient statistics (specifically momentum vectors) in each layer into K groups using online clustering algorithms such as mini-batch K-means.</li>
                <li>Cluster-specific Learning Rate Scaling: After grouping, SGG calculates a scaling factor for each cluster based on the deviation of the cluster's statistics relative to the layer's and the entire model's global statistics. This scaling factor modulates the learning rate for each parameter in the cluster.</li>
                <li>Parameter Update: The scaled learning rates are then used to update the model parameters, ensuring that each parameter adapts its learning rate based on its group's characteristics while maintaining individual parameter adaptation.</li>
            </ol>
        </div>
    </div>
</section>

<section class="section" id="Experiments" style="padding-top:1.5rem;">
    <div class="container" style="max-width: 80%; margin: 0 auto;">
        <h1 class="title is-3" style="text-align:center; font-size: 2.3em; margin-bottom:1rem;">Experiments</h1>
        <br>
        <div class="content has-text-justified" style="max-width: 85%; font-size: 1.2em; margin: 0 auto; line-height: 1.55;">
            <p style="text-align:justify; color:rgb(31, 30, 30);">
                To thoroughly evaluate the effectiveness of Scaling with Gradient Grouping (SGG), we conducted a comprehensive set of experiments across various benchmarks and tasks. The results demonstrate that SGG consistently improves performance and accelerates convergence when integrated with different optimizers and models.
            </p>
        </div>
        <br>
        <div class="content has-text-justified" style="max-width: 85%; font-size: 1.2em; margin: 0 auto; line-height: 1.55;">
            <h2 style="text-align:left; font-size: 1.5em; margin-bottom:0.5rem;">3.2 Comparison Results with LLMs</h2>
            <p style="text-align:justify; color:rgb(31, 30, 30);">
                SGG was evaluated on various LLM benchmarks, including pre-training on C4, supervised fine-tuning (SFT) on GLUE, PEFT on commonsense reasoning tasks, and Direct Preference Optimization (DPO). The results show that SGG consistently delivers performance gains and faster convergence across different model sizes and settings.
            </p>
            <br>
            <h3 style="text-align:left; font-size: 1.4em; margin-bottom:0.5rem;">Pre-training on C4</h3>
            <p style="text-align:justify; color:rgb(31, 30, 30);">
                Table 4 shows the validation perplexity (PPL) for different optimizers and model sizes during pre-training on the C4 dataset. SGG consistently reduces validation perplexity compared to baselines, with significant gains across various model sizes.
            </p>
            <table class="table is-bordered is-striped" style="width: 100%; margin-bottom: 1rem;">
                <thead>
                    <tr>
                        <th>Method</th>
                        <th>Venue</th>
                        <th>60M</th>
                        <th>130M</th>
                        <th>350M</th>
                        <th>1B</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Adam</td>
                        <td>ICLR'15</td>
                        <td>34.06</td>
                        <td>25.08</td>
                        <td>18.80</td>
                        <td>15.56</td>
                    </tr>
                    <tr>
                        <td>NAdam</td>
                        <td>ICLR'18</td>
                        <td>35.86</td>
                        <td>28.88</td>
                        <td>19.24</td>
                        <td>15.78</td>
                    </tr>
                    <tr>
                        <td>RAdam</td>
                        <td>ICLR'20</td>
                        <td>30.43</td>
                        <td>25.17</td>
                        <td>19.13</td>
                        <td>15.65</td>
                    </tr>
                    <tr>
                        <td>LAMB</td>
                        <td>ICLR'20</td>
                        <td>33.04</td>
                        <td>24.37</td>
                        <td>18.26</td>
                        <td>15.84</td>
                    </tr>
                    <tr>
                        <td>Adan</td>
                        <td>TPAMI'23</td>
                        <td>32.01</td>
                        <td>23.14</td>
                        <td>17.32</td>
                        <td>14.70</td>
                    </tr>
                    <tr>
                        <td>Adam+SGG</td>
                        <td>Ours</td>
                        <td><strong>30.31</strong> (-3.75)</td>
                        <td><strong>22.18</strong> (-2.90)</td>
                        <td><strong>17.28</strong> (-1.52)</td>
                        <td><strong>14.30</strong> (-1.26)</td>
                    </tr>
                </tbody>
            </table>
            <p style="text-align:justify; color:rgb(31, 30, 30);">
                <strong>Table 4:</strong> C4 Pre-training with diverse LLaMA sizes (from 60M to 1B). Comparison of full-rank, memory-efficient, and low-rank optimizers. Validation Perplexity (PPL%↓: lower is better) is reported. Bold and green types denote the best results and gains↓ of SGG (blue background) over related baselines (gray background).
            </p>
            <br>
            <h3 style="text-align:left; font-size: 1.4em; margin-bottom:0.5rem;">SFT on GLUE</h3>
            <p style="text-align:justify; color:rgb(31, 30, 30);">
                Table 5 shows the results of fine-tuning RoBERTa-base models on the GLUE benchmark. SGG consistently improves performance across various tasks and settings, demonstrating its versatility and robustness.
            </p>
            <table class="table is-bordered is-striped" style="width: 100%; margin-bottom: 1rem;">
                <thead>
                    <tr>
                        <th>Optimizer</th>
                        <th>Rank</th>
                        <th>CoLA</th>
                        <th>STS-B</th>
                        <th>MRPC</th>
                        <th>RTE</th>
                        <th>SST2</th>
                        <th>MNLI</th>
                        <th>QNLI</th>
                        <th>QQP</th>
                        <th>Average</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>AdamW</td>
                        <td>Full</td>
                        <td>62.24</td>
                        <td>90.92</td>
                        <td>91.30</td>
                        <td>79.42</td>
                        <td>94.57</td>
                        <td>87.18</td>
                        <td>92.33</td>
                        <td>92.28</td>
                        <td>86.24</td>
                    </tr>
                    <tr>
                        <td>AdamW+SGG</td>
                        <td>Full</td>
                        <td><strong>63.36</strong> (+1.12)</td>
                        <td><strong>91.22</strong> (+0.30)</td>
                        <td><strong>92.65</strong> (+1.35)</td>
                        <td><strong>80.87</strong> (+1.45)</td>
                        <td><strong>95.58</strong> (+1.01)</td>
                        <td><strong>88.32</strong> (+1.14)</td>
                        <td><strong>92.88</strong> (+0.55)</td>
                        <td><strong>93.32</strong> (+1.04)</td>
                        <td><strong>87.28</strong> (+1.00)</td>
                    </tr>
                </tbody>
            </table>
            <p style="text-align:justify; color:rgb(31, 30, 30);">
                <strong>Table 5:</strong> GLUE Benchmark Results with RoBERTa-base. Top-1 accuracy (%↑: higher is better) is reported. Comparison across both full-rank and low-rank (LoRA r = 4, r = 8) settings. Bold and green types denote the best results and performance gains↑ of SGG (blue background) compared to related baselines (gray background).
            </p>
            <br>
            <h3 style="text-align:left; font-size: 1.4em; margin-bottom:0.5rem;">PEFT on Commonsense Reasoning</h3>
            <p style="text-align:justify; color:rgb(31, 30, 30);">
                Table 6 shows the results of evaluating SGG on commonsense reasoning tasks using the LLaMA-7B model. SGG consistently improves performance across all tasks, achieving significant gains over baselines.
            </p>
            <table class="table is-bordered is-striped" style="width: 100%; margin-bottom: 1rem;">
                <thead>
                    <tr>
                        <th>Method</th>
                        <th>BoolQ</th>
                        <th>PIQA</th>
                        <th>SIQA</th>
                        <th>WG</th>
                        <th>Arc-E</th>
                        <th>OBQA</th>
                        <th>Average</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>LoRA</td>
                        <td>68.9</td>
                        <td>80.7</td>
                        <td>77.4</td>
                        <td>78.8</td>
                        <td>77.8</td>
                        <td>74.8</td>
                        <td>74.7</td>
                    </tr>
                    <tr>
                        <td>LoRA+SGG</td>
                        <td><strong>70.3</strong> (+1.4)</td>
                        <td><strong>83.6</strong> (+2.9)</td>
                        <td><strong>78.8</strong> (+1.4)</td>
                        <td><strong>81.7</strong> (+2.9)</td>
                        <td><strong>81.5</strong> (+3.7)</td>
                        <td><strong>79.0</strong> (+4.2)</td>
                        <td><strong>77.6</strong> (+2.9)</td>
                    </tr>
                </tbody>
            </table>
            <p style="text-align:justify; color:rgb(31, 30, 30);">
                <strong>Table 6:</strong> LLaMA-7B PEFT Results on Commonsense Reasoning. Comparison of LoRA+SGG (blue background) against baselines. Top-1 accuracy (%↑: higher is better) of selected tasks and all tasks on average (Avg.) are reported. Bold and green types denote the best results and gains↑ compared to LoRA (gray background).
            </p>
            <br>
            <h3 style="text-align:left; font-size: 1.4em; margin-bottom:0.5rem;">DPO</h3>
            <p style="text-align:justify; color:rgb(31, 30, 30);">
                Table 7 shows the results of Direct Preference Optimization (DPO) using the Qwen2.5-0.5B model. SGG consistently improves performance across different optimizers and settings.
            </p>
            <table class="table is-bordered is-striped" style="width: 100%; margin-bottom: 1rem;">
                <thead>
                    <tr>
                        <th>Optimizer</th>
                        <th>Full-Rank</th>
                        <th>LoRA</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>SGD</td>
                        <td>70.10</td>
                        <td>69.73</td>
                    </tr>
                    <tr>
                        <td>AdamW</td>
                        <td>71.39</td>
                        <td>70.22</td>
                    </tr>
                    <tr>
                        <td>LAMB</td>
                        <td>70.82</td>
                        <td>70.39</td>
                    </tr>
                    <tr>
                        <td>SGD+SGG</td>
                        <td><strong>70.82</strong> (+0.72)</td>
                        <td><strong>70.76</strong> (+1.03)</td>
                    </tr>
                    <tr>
                        <td>AdamW+SGG</td>
                        <td><strong>71.85</strong> (+0.47)</td>
                        <td><strong>72.02</strong> (+1.80)</td>
                    </tr>
                    <tr>
                        <td>LAMB+SGG</td>
                        <td><strong>71.32</strong> (+0.50)</td>
                        <td><strong>71.28</strong> (+0.89)</td>
                    </tr>
                </tbody>
            </table>
            <p style="text-align:justify; color:rgb(31, 30, 30);">
                <strong>Table 7:</strong> Qwen2.5-0.5B DPO Results with full-rank and LoRA setups. Top-1 accuracy(%)↑ is reported. Bold and green types denote best results and relative gains.
            </p>
            <br>
            <h2 style="text-align:left; font-size: 1.5em; margin-bottom:0.5rem;">3.3 Comparison Results with MLLMs</h2>
            <p style="text-align:justify; color:rgb(31, 30, 30);">
                SGG was also evaluated on various multimodal LLM benchmarks, including VQA and MLLM evaluation benchmarks. The results demonstrate that SGG consistently improves performance across different tasks and settings.
            </p>
            <br>
            <h3 style="text-align:left; font-size: 1.4em; margin-bottom:0.5rem;">Full-Rank SFT</h3>
            <p style="text-align:justify; color:rgb(31, 30, 30);">
                Table 8 shows the results of supervised fine-tuning (SFT) on various MLLM benchmarks using full-rank optimizers. SGG consistently improves performance across different tasks and optimizers.
            </p>
            <table class="table is-bordered is-striped" style="width: 100%; margin-bottom: 1rem;">
                <thead>
                    <tr>
                        <th>Method</th>
                        <th>Image Question Answering Benchmarks</th>
                        <th>Avg.</th>
                        <th>GQA</th>
                        <th>VizWiz</th>
                        <th>SciVQAI</th>
                        <th>VQAT</th>
                        <th>MMB</th>
                        <th>MMBCN</th>
                        <th>POPE</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>LLaVA-v1.5 AdamW</td>
                        <td>62.0</td>
                        <td>50.0</td>
                        <td>66.8</td>
                        <td>58.2</td>
                        <td>64.3</td>
                        <td>58.3</td>
                        <td>85.9</td>
                        <td>63.6</td>
                    </tr>
                    <tr>
                        <td>LLaVA-v1.5 Adafactor</td>
                        <td>62.7</td>
                        <td>48.2</td>
                        <td>70.7</td>
                        <td>57.1</td>
                        <td>66.1</td>
                        <td>60.4</td>
                        <td>86.0</td>
                        <td>64.5</td>
                    </tr>
                    <tr>
                        <td>LLaVA-v1.5 LAMB</td>
                        <td>43.8</td>
                        <td>53.3</td>
                        <td>61.5</td>
                        <td>43.4</td>
                        <td>43.2</td>
                        <td>41.8</td>
                        <td>81.2</td>
                        <td>52.6</td>
                    </tr>
                    <tr>
                        <td>LLaVA-v1.5 AdamW+SGG</td>
                        <td><strong>62.4</strong> (+0.4)</td>
                        <td><strong>50.2</strong> (+0.2)</td>
                        <td><strong>69.8</strong> (+3.0)</td>
                        <td><strong>57.4</strong> (-0.8)</td>
                        <td><strong>65.9</strong> (+1.6)</td>
                        <td><strong>60.1</strong> (+1.8)</td>
                        <td><strong>86.3</strong> (+0.4)</td>
                        <td><strong>64.6</strong> (+1.0)</td>
                    </tr>
                </tbody>
            </table>
            <p style="text-align:justify; color:rgb(31, 30, 30);">
                <strong>Table 8:</strong> MLLM performance comparison on diverse benchmarks with LLaVA variants and different optimizers. Top-1 accuracy (%)↑ for selected tasks and all-task averaged (Avg.) results are reported. MMB and MMBCN denote MMbench and MMbench (Chinese). Bold and green types denote the best results and gains↓ of SGG (blue background) over related baselines (gray background). Please view Table A6 for the full results.
            </p>
        </div>
    </div>
</section>
  
</section>
  <!-- BibTeX Section -->
  <section class="section" id="BibTeX" style="padding-top:1.5rem;">
    <div class="container is-max-desktop content" style="max-width: 80%; margin: 0 auto -0.1rem auto;">
      <h1 class="title is-3" style="text-align:center; font-size: 2.3em;">BibTeX</h1>
      <pre style="font-size: 1.0em;">
        @inproceedings{acl2025sgg,
          title={Taming LLMs with Gradient Grouping},
          author={Li, Siyuan and Tian, Juanxi and Wang, Zedong and Jin, Xin and Liu, Zicheng and Zhang, Wentao and Xu, Dan},
          booktitle={Annual Meeting of the Association for Computational Linguistics},
          year={2025}
       }
      </pre>
    </div>
  </section>
