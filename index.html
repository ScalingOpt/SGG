<!DOCTYPE html>
<html>

<head>
  <link rel="icon" type="image/png" href="website/img/page_logo.png">

  <title>SGG: Taming LLMs by Scaling Learning Rates with Gradient Grouping</title>

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.0/dist/chart.umd.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-datalabels@2.0.0"></script>
    <script
        src="https://cdn.jsdelivr.net/npm/chartjs-plugin-annotation@3.0.1/dist/chartjs-plugin-annotation.min.js"></script>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
    <link rel="stylesheet" href="website/css/bulma.min.css">
    <link rel="stylesheet" href="website/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="website/css/bulma-slider.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="website/css/fontawesome.all.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="website/javascript/bulma-carousel.min.js"></script>
    <script src="website/javascript/bulma-slider.min.js"></script>
    <script src="website/javascript/explorer-index.js"></script>

    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet"
        integrity="sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"
        integrity="sha384-ka7Sk0Gln4gmtz2MlQnikT1wXgYsOg+OMhuP+IlRH9sENBO0LRn5q+8nbTov4+1p"
        crossorigin="anonymous"></script>

    <link href="https://unpkg.com/tabulator-tables@5.5.2/dist/css/tabulator_bootstrap4.min.css" rel="stylesheet">
    <script type="text/javascript" src="https://unpkg.com/tabulator-tables@5.5.2/dist/js/tabulator.min.js"></script>
    <script defer src="website/javascript/fontawesome.all.min.js"></script>
    <!-- <script src="website/javascript/peity-vanilla.js"></script> -->


    <!-- below we load some js scripts -->
    <script src="website/javascript/benchmark_table.js" type="module"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <link rel="stylesheet" href="website/css/index.css">

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C7GJ4FYMY9"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());

        gtag('config', 'G-C7GJ4FYMY9');
    </script>

    <!-- MathJax script -->
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
    <script type="text/javascript">
        MathJax.Hub.Config({
            tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
        });
    </script>

    <noscript>
        <p><img alt="Clicky" width="1" height="1" src="https://in.getclicky.com/101339888ns.gif" /></p>
    </noscript>

    <script>
        document.addEventListener('DOMContentLoaded', function () {
        var toggles = document.querySelectorAll('.toggle-section');
        toggles.forEach(function(toggle) {
            toggle.addEventListener('click', function() {
            var content = document.getElementById(toggle.getAttribute('aria-controls'));
            var toggleIcon = toggle.children[1].children[0];
            content.classList.toggle('is-active');
            if (content.classList.contains('is-active')) {
                toggleIcon.style.transition = 'transform 0.3s ease';
                toggleIcon.style.transform = 'rotate(180deg)';
            } else {
                toggleIcon.style.transition = 'transform 0.3s ease';
                toggleIcon.style.transform = 'rotate(0deg)';
            }
            });
        });
        });
      </script>

    <style>
        .collapse-content {
          display: none;
          margin-top: 10px;
        }
        .collapse-content.is-active {
          display: block;
        }
        /* .toggle-section .icon.is-small {
          transition: transform 0.3s ease;
        } */
        /* .toggle-section .fa-angle-up {
          transform: rotate(180deg);
        } */
      </style>
</head>

<body>

  <!-- Single-line Title with Bulma's .title.is-1 is-bold (centered heading) -->
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">

            <!-- MAIN TITLE (logo + "AlphaOne:") -->
            <!-- TITLE -->
            <h1 class="title is-1 publication-title is-bold" style="display: flex; align-items: center; justify-content: center; gap: 10px;">
              <!-- Left: ACL Image -->
              <img src="website/img/acl.png" alt="ACL Logo" style="height: 1em; vertical-align: middle;" />

              <!-- Center: SGG Text -->
              <span class="alphaone" style="vertical-align: middle;">SGG</span>

              <!-- Right: 2025 Main * Label -->
              <span style="font-size: 0.6em; color: #555; vertical-align: middle;">2025 Main ⭐️</span>
            </h1>

            <!-- SUBTITLE -->
            <h2 class="title is-1 publication-title is-bold" style="margin-top: -0.5em; margin-right: 30px; margin-left: 30px; text-align: center;">
              Taming LLMs by Scaling Learning Rates with Gradient Grouping
            </h2>
            <!-- Author list -->
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://lupin1998.github.io"   target="_blank">Siyuan Li*</a>
                <sup style="display: inline-flex; align-items: center; margin-left: -0.3em; gap: 0.05em;">
                  <img src="website/img/zhejiang.png" alt="zhejiang" style="height: 1.2em; vertical-align: baseline; margin: 0 0.1em;">
                </sup>&nbsp;&nbsp;&nbsp;
              </span>
              <span class="author-block">
                <a href="https://github.com/tianshijing"   target="_blank">Juanxi Tian*</a>
                <sup style="display: inline-flex; align-items: center; margin-left: -0.3em; gap: 0.05em;">
                  <img src="website/img/pku.png" alt="pku" style="height: 1.2em; vertical-align: baseline; margin: 0 0.1em;">
                </sup>&nbsp;&nbsp;&nbsp;
              </span>
              <span class="author-block">
                <a href="https://jacky1128.github.io"   target="_blank">Zedong Wang*</a>
                <sup style="display: inline-flex; align-items: center; margin-left: -0.3em; gap: 0.05em;">
                  <img src="website/img/hkust.png" alt="hkust" style="height: 1.2em; vertical-align: baseline; margin: 0 0.1em;">
                </sup>&nbsp;&nbsp;&nbsp;
              </span>
              <span class="author-block">
                <a href="https://jinxins.github.io"   target="_blank">Xin Jin</a>
                <sup style="display: inline-flex; align-items: center; margin-left: -0.3em; gap: 0.05em;">
                  <img src="website/img/westlake.png" alt="westlake" style="height: 1.2em; vertical-align: baseline; margin: 0 0.1em;">
                </sup>&nbsp;&nbsp;&nbsp;
              </span><br>
              <span class="author-block">
                <a href="">Zicheng Liu</a>
                <sup style="display: inline-flex; align-items: center; margin-left: -0.3em; gap: 0.05em;">
                  <img src="website/img/zhejiang.png" alt="zhejiang" style="height: 1.2em; vertical-align: baseline; margin: 0 0.1em;">
                </sup>&nbsp;&nbsp;&nbsp;
              </span>
              <span class="author-block">
                <a href="https://zwt233.github.io">Wentao Zhang</a>
                <sup style="display: inline-flex; align-items: center; margin-left: -0.3em; gap: 0.05em;">
                  <img src="website/img/pku.png" alt="pku" style="height: 1.2em; vertical-align: baseline; margin: 0 0.1em;">
                </sup>&nbsp;&nbsp;&nbsp;                
              </span>
              <span class="author-block">
                <a href="https://www.danxurgb.net">Dan Xu</a>
                <sup style="display: inline-flex; align-items: center; margin-left: -0.3em; gap: 0.05em;">
                  <img src="website/img/hkust.png" alt="hkust" style="height: 1.2em; vertical-align: baseline; margin: 0 0.1em;">
                </sup>&nbsp;&nbsp;&nbsp;
              </span>
            </div>
            <div class="is-size-5 publication-authors" style="margin-top:1em;">
              <span class="author-block">
                <sup style="display: inline-flex; align-items: center; margin-left: -0.3em; gap: 0.05em;">
                  <img src="website/img/zhejiang.png" alt="zhejiang" style="height: 1.2em; vertical-align: baseline; margin: 0 0.1em;">
                </sup>Zhejiang University</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
              <span class="author-block">
                <sup style="display: inline-flex; align-items: center; margin-left: -0.3em; gap: 0.05em;">
                  <img src="website/img/westlake.png" alt="westlake" style="height: 1.2em; vertical-align: baseline; margin: 0 0.1em;">
                </sup>Westlake University</span>
              <span class="author-block">
                <sup style="display: inline-flex; align-items: center; margin-left: -0.3em; gap: 0.05em;">
                  <img src="website/img/hkust.png" alt="hkust" style="height: 1.2em; vertical-align: baseline; margin: 0 0.1em;">
                </sup>The Hong Kong University of Science and Technology</span>
              <span class="author-block">
                <sup style="display: inline-flex; align-items: center; margin-left: -0.3em; gap: 0.05em;">
                  <img src="website/img/pku.png" alt="pku" style="height: 1.2em; vertical-align: baseline; margin: 0 0.1em;">
                </sup>Peking University</span>
            </div>
            <!-- Links -->
            <div class="column has-text-centered" style="margin-top:1.5em;">
              <span class="link-block">
                <a href="https://arxiv.org/abs/2506.01049" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>Paper</span>
                </a>&nbsp;&nbsp;
              </span>
              <span class="link-block">
                <a href="https://github.com/ScalingOpt/SGG" class="external-link button is-normal is-rounded is-dark" role="button" target="_blank">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code (Coming soon)</span>
                </a> &nbsp;&nbsp;
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container" style="margin-top: -6rem; margin-bottom: -4rem; max-width: 80%;">
      <div class="columns is-centered m-6">
        <div class="column is-full">
          <!-- 使用 flex 布局保持内容居中 -->
          <div class="box p-4" style="width: 80%; height: 90%; margin: 0 auto; display: flex; align-items: center;">
            <div class="columns is-mobile is-vcentered" style="display: table-row; margin: 0 auto; width: 100%;">
              <!-- 图片列 -->
              <div class="column is-half" style="display: table-cell; vertical-align: middle;">
                <div style="text-align: right; padding-right: -1rem;">
                  <img src="website/img/SGG_1.png" alt="alphaone-teaser" 
                       style="max-width: 90%; margin-right: -0.3rem;" />
                  <!-- 添加 caption -->
                  <p style="font-size: 0.9rem; color: #555; margin-top: 0.5rem; text-align: center;">
                    Scaling with Gradient Grouping. Illustration of SGG with online grouping and group-specific learning rate (LR) scaling upon adaptive LR optimizers.
                  </p>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
<!-- <p style="text-align: left; margin: 0; color:rgb(60, 56, 56);"> 
                  <b>Figure 1.</b> <b>Conceptual illustration</b> of reasoning modulation strategies. 
                  Our <span style="font-size: 0.9em;">\(\alpha\)1</span> employs a <i>slow-to-fast</i> reasoning schedule controlled by <span style="font-size: 0.9em;">\(\alpha\)</span>. 
                  <span style="font-size: 0.9em;">\(\alpha\)1</span> scales more efficiently than <i>monotonously increasing</i> method s1 (<span style="color: #dbbf65; font-weight: bold; font-size: 1.0em;">yellow</span>) and generally outperforms <i>monotonously decreasing</i> (<span style="color: #a36ca0; font-weight: bold; font-size: 1.0em;">purple</span>) approaches.
                </p> -->
  <!-- Overview Section -->
  <section class="section" id="overview" style="padding-top:1.5rem;">
    <div class="container" style="max-width: 80%; margin: 0 auto;">
      <h1 class="title is-3" style="text-align:center; font-size: 2.3em; margin-bottom:1rem;">Overview</h1>
      <br>
      <!-- 图片单独居中展示，并缩小 -->
      <div style="text-align: center; margin: 2rem 0;">
        <img src="website/img/SGG_2.png" alt="SGG Framework Overview" style="width: 50%; height: auto; display: inline-block;">
      </div>
      <div class="content has-text-justified" style="max-width: 85%; font-size: 1.2em; margin: 0 auto; line-height: 1.55;">
        <p style="text-align:justify; color:rgb(31, 30, 30);">
          Training large language models (LLMs) poses challenges due to their massive scale and heterogeneous architectures. While adaptive optimizers like AdamW help address gradient variations, they still struggle with efficient and effective parameter-wise learning rate estimation, resulting in training instability, slow convergence, and poor compatibility with parameter-efficient fine-tuning (PEFT) techniques. This work introduces <span class="sgg">Scaling with Gradient Grouping</span> (SGG), an optimizer wrapper that improves adaptive learning rate estimation by dynamic grouping and group-specific scaling. SGG first groups gradient statistics in each layer into clusters and then applies cluster-specific scaling to calibrate learning rates for each parameter, thus imposing collective group-wise constraints while maintaining precise per-parameter adaptation.
        </p>
  
      </div>
    </div>
  </section>

  <section class="section" id="Method" style="padding-top:1.5rem;">
    <div class="container is-max-desktop content" style="max-width: 80%; margin: 0 auto -0.1rem auto;">
        <h1 class="title is-3" style="text-align:center; font-size: 2.3em;">Method</h1>
        <p>
            Training large language models (LLMs) poses challenges due to their massive scale and heterogeneous architectures. While adaptive optimizers like AdamW help address gradient variations, they still struggle with efficient and effective parameter-wise learning rate estimation, resulting in training instability, slow convergence, and poor compatibility with parameter-efficient fine-tuning (PEFT) techniques. This work introduces Scaling with Gradient Grouping (SGG), an optimizer wrapper that improves adaptive learning rate estimation by dynamic grouping and group-specific scaling.
        </p>
        <p>
            SGG first groups gradient statistics in each layer into clusters and then applies cluster-specific scaling to calibrate learning rates for each parameter, thus imposing collective group-wise constraints while maintaining precise per-parameter adaptation. Experiments on diverse (M)LLM benchmarks show that SGG integrates seamlessly with existing optimizers, and offers consistent gains and faster convergence over baselines, with various model sizes. Its stability across varying batch sizes and learning rates establishes SGG as a robust choice for LLM optimization.
        </p>
        <p>
            To demonstrate the plug-and-play integration of our SGG, we first outline the essential steps in gradient-based optimizers. The process typically begins with gradient computation. At iteration <math>t</math>, the gradient <math>g_t^l</math> of objective <math>L</math> w.r.t. parameters <math>\theta_{t-1}^l</math> of layer <math>l</math> is calculated as:
            <math>g_t^l = \nabla_{\theta_{t-1}^l} L(\theta_{t-1}^l, D)</math>
            where <math>D</math> denotes the training dataset, and <math>\theta_{t-1}^l</math> is from the previous iteration. Subsequently, historical gradient information is incorporated to stabilize the update, commonly referred to as momentum <math>m_t^l</math>. While vanilla SGD uses the current gradient instead (<math>m_t^l = g_t^l</math>), momentum-based methods often employ an exponential moving average (EMA) to smooth estimates over time:
            <math>m_t^l = \text{MomentumEstimate}(g_t^l, m_{t-1}^l, \beta_1)</math>
            where <math>m_{t-1}^l</math> is from the last iteration, and the EMA decay <math>\beta_1</math> controls the retention of past gradients.
        </p>
        <p>
            Adaptive learning rate algorithms further refine the process by calculating parameter-wise or layer-wise second-moment estimates of gradients to calibrate step sizes:
            <math>\alpha_t^l = \text{LREstimate}(\alpha_{t-1}^l, m_t^l, \beta_2, \eta_t)</math>
            where <math>\eta_t</math> indicates the global learning rate set by scheduler at iteration <math>t</math>, and <math>\beta_2</math> is the EMA decay like <math>\beta_1</math>. Non-adaptive methods, in contrast, simply use the global one instead (<math>\alpha_t^l = \eta_t</math>). Note that this learning rate adaptation typically increases memory overhead, particularly for large-scale models – a key challenge that most prior works aim to address.
        </p>
        <p>
            SGG begins with dynamic grouping as GradCluster(<math>m_t^l</math>, <math>K</math>), which partitions momentum vectors <math>m_t^l</math> within each layer <math>l</math> into <math>K</math> groups with related indices <math>C_t^l</math> according to their similarity. To achieve this, online clustering stands out as a straightforward solution, and the choice of specific clustering algorithms is then crucial for both effectiveness and efficiency. As such, we evaluate several potential strategies, including K-means, mini-batch K-means, Gaussian Mixture Models (GMM), and DBSCAN. Ablation studies show that mini-batch K-means offer the most favorable trade-off between clustering quality and computational efficiency. Thus, we select this as the default clustering implementation of GradCluster(<math>m_t^l</math>, <math>K</math>) in SGG.
        </p>
        <p>
            We introduce ScaleUpdate(<math>C_t^l</math>, <math>g_t^l</math>, <math>\beta_3</math>) to calculate the scaling factor <math>S_t^l[c]</math> for each cluster <math>c</math> after grouping, which modulates learning rate <math>\alpha_t^l</math>. This involves two sub-tasks: (i) measuring the statistics for different levels of partitions (e.g., clusters, layers, and even the global one); (ii) updating cluster-specific scales <math>S_t^l[c]</math> based on the above statistics. This contrasts with the previous Adam-mini, which replaces per-parameter adaptive learning rates with their group-wise means directly.
        </p>
    </div>
</section>
  <!-- BibTeX Section -->
  <section class="section" id="BibTeX" style="padding-top:1.5rem;">
    <div class="container is-max-desktop content" style="max-width: 80%; margin: 0 auto -0.1rem auto;">
      <h1 class="title is-3" style="text-align:center; font-size: 2.3em;">BibTeX</h1>
      <pre style="font-size: 1.0em;">
        @inproceedings{acl2025sgg,
          title={Taming LLMs with Gradient Grouping},
          author={Li, Siyuan and Tian, Juanxi and Wang, Zedong and Jin, Xin and Liu, Zicheng and Zhang, Wentao and Xu, Dan},
          booktitle={Annual Meeting of the Association for Computational Linguistics},
          year={2025}
       }
      </pre>
    </div>
  </section>

