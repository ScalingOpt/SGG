<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>SGG Optimizer — ACL 2025</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css" />
  <style>
    body {
      font-family: "Times New Roman", Times, serif;
      margin: 0;
      padding: 0;
      background: #f9f9f9;
      color: #1a1a1a;
    }
    header {
      background-color: #0b0b0b;
      color: white;
      padding: 2rem 3rem;
      display: flex;
      justify-content: space-between;
      align-items: center;
      flex-wrap: wrap;
    }
    header h1 {
      margin: 0;
      font-size: 2rem;
    }
    nav a {
      color: white;
      text-decoration: none;
      margin-left: 1.5rem;
      font-size: 1rem;
    }
    nav a:hover {
      text-decoration: underline;
    }
    .hero {
      text-align: center;
      padding: 4rem 2rem;
      background: linear-gradient(135deg, #eaeaea, #fdfdfd);
    }
    .hero h2 {
      font-size: 2.5rem;
      margin-bottom: 1rem;
    }
    .buttons {
      margin-top: 1.5rem;
    }
    .buttons a {
      display: inline-block;
      margin: 0 1rem;
      padding: 0.75rem 1.5rem;
      border-radius: 5px;
      font-size: 1rem;
      text-decoration: none;
      transition: background 0.3s;
    }
    .arxiv {
      background-color: #b31b1b;
      color: white;
    }
    .arxiv:hover {
      background-color: #a01a1a;
    }
    .github {
      background-color: #24292e;
      color: white;
    }
    .github:hover {
      background-color: #1b1f23;
    }
    .content {
      max-width: 960px;
      margin: 2rem auto;
      padding: 0 1.5rem;
    }
    .section {
      margin-bottom: 3rem;
    }
    .section h3 {
      font-size: 1.8rem;
      margin-bottom: 1rem;
      border-bottom: 1px solid #ccc;
      padding-bottom: 0.3rem;
    }
    .section table {
      width: 100%;
      border-collapse: collapse;
      margin-top: 1rem;
      font-size: 0.95rem;
    }
    .section th, .section td {
      border: 1px solid #ddd;
      padding: 0.75rem;
      text-align: center;
    }
    .section th {
      background-color: #f0f0f0;
    }
    footer {
      text-align: center;
      padding: 2rem;
      background: #f0f0f0;
      margin-top: 3rem;
    }
  </style>
</head>
<body>
  <header>
    <h1>SGG Optimizer</h1>
    <nav>
      <a href="#introduction">Introduction</a>
      <a href="#method">Method</a>
      <a href="#results">Results</a>
      <a href="#ablation">Ablation</a>
      <a href="#dpo">DPO</a>
      <a href="#mllm">MLLM</a>
      <a href="#robustness">Robustness</a>
    </nav>
  </header>

  <section class="hero">
    <h2>Taming LLMs by Scaling Learning Rates with Gradient Grouping</h2>
    <p>ACL 2025 · Siyuan Li, Juanxi Tian, Zedong Wang, Xin Jin, Zicheng Liu†, Wentao Zhang, Dan Xu</p>
    <div class="buttons">
      <a href="https://arxiv.org/pdf/2506.01049" target="_blank" class="arxiv"><i class="fas fa-file-pdf"></i> Paper (arXiv)</a>
      <a href="https://github.com/ScalingOpt/SGG" target="_blank" class="github"><i class="fab fa-github"></i> Code (GitHub)</a>
    </div>
  </section>

  <main class="content">
    <section id="introduction" class="section">
      <h3>Introduction</h3>
      <p>SGG is a plug-and-play optimizer wrapper that improves the adaptive learning rate estimation by introducing online gradient grouping and group-specific scaling. It stabilizes training, accelerates convergence, and enhances compatibility with PEFT approaches.</p>
    </section>

    <section id="method" class="section">
      <h3>Methodology</h3>
      <p><strong>Clustering:</strong> Momentum vectors in each layer are clustered using mini-batch K-means. This allows identification of optimization-relevant structures.</p>
      <p><strong>Scaling:</strong> Cluster-specific learning rate scaling is derived using the Median Deviation to Average (MDA), comparing local vs. global cluster variability. It promotes homogeneity while retaining parameter-wise precision.</p>
      <p><strong>Integration:</strong> SGG can be seamlessly wrapped around any optimizer (e.g., Adam, LAMB, CAME) and works with full-rank or LoRA training without modifying model architecture.</p>
    </section>

    <section id="results" class="section">
      <h3>LLM Pretraining Results</h3>
      <p>SGG improves validation perplexity on C4 across model scales. It reduces PPL significantly vs. baseline optimizers.</p>
      <table>
        <tr><th>Method</th><th>60M</th><th>130M</th><th>350M</th><th>1B</th></tr>
        <tr><td>Adam</td><td>34.06</td><td>25.08</td><td>18.80</td><td>15.56</td></tr>
        <tr><td><strong>Adam+SGG</strong></td><td><strong>30.31</strong></td><td><strong>22.18</strong></td><td><strong>17.28</strong></td><td><strong>14.30</strong></td></tr>
      </table>
    </section>

    <section id="dpo" class="section">
      <h3>DPO Results</h3>
      <p>SGG enhances human preference alignment with DPO. It improves LoRA performance and even surpasses full-rank counterparts.</p>
      <table>
        <tr><th>Optimizer</th><th>Full-Rank</th><th>LoRA</th></tr>
        <tr><td>AdamW</td><td>71.39</td><td>70.22</td></tr>
        <tr><td><strong>AdamW+SGG</strong></td><td><strong>71.85</strong></td><td><strong>72.02</strong></td></tr>
      </table>
    </section>

    <section id="mllm" class="section">
      <h3>MLLM Performance</h3>
      <p>Evaluated on GQA, VQAv2, VizWiz, SciVQAI, and others, SGG boosts LLaVA-based performance in both full and low-rank settings.</p>
      <table>
        <tr><th>Optimizer</th><th>VizWiz</th><th>MMBench</th><th>POPE</th><th>Average</th></tr>
        <tr><td>AdamW</td><td>50.0</td><td>64.3</td><td>85.9</td><td>63.6</td></tr>
        <tr><td><strong>AdamW+SGG</strong></td><td><strong>51.0</strong></td><td><strong>65.9</strong></td><td><strong>86.3</strong></td><td><strong>64.6</strong></td></tr>
      </table>
    </section>

    <section id="robustness" class="section">
      <h3>Robustness & Stability</h3>
      <p>SGG offers superior robustness to extreme learning rates and large batch sizes. It avoids divergence and maintains low loss across configurations.</p>
      <ul>
        <li>Batch size tested up to 4096</li>
        <li>Learning rate tested up to 0.1</li>
        <li>Stable convergence with minimal tuning</li>
      </ul>
    </section>

    <section id="ablation" class="section">
      <h3>Ablation Studies</h3>
      <p>We study clustering parameters and EMA decay in depth. Optimal settings found:</p>
      <ul>
        <li><strong>Number of clusters:</strong> K = 3</li>
        <li><strong>Recluster interval:</strong> T = 500</li>
        <li><strong>EMA decay:</strong> β₃ = 0.99</li>
      </ul>
    </section>

  </main>

  <footer>
    &copy; 2025 ScalingOpt | Optimizer for Large Models. Font: Times New Roman.
  </footer>
</body>
</html>
